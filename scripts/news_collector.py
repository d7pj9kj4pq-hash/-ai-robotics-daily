#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AIæœºå™¨äººèµ„è®¯è‡ªåŠ¨æ”¶é›†å™¨ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
"""

import requests
import feedparser
import yaml
import json
import re
from datetime import datetime
import time
import os

class NewsCollector:
    def __init__(self):
        with open('config/sources.yaml', 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)
        
        os.makedirs('output/daily', exist_ok=True)
    
    def clean_html_tags(self, text):
        """æ¸…ç†HTMLæ ‡ç­¾å’Œæ ¼å¼"""
        if not text:
            return ""
        
        # ç§»é™¤HTMLæ ‡ç­¾
        text = re.sub(r'<[^>]+>', '', text)
        
        # æ›¿æ¢HTMLå®ä½“
        html_entities = {
            '&nbsp;': ' ', '&amp;': '&', '&lt;': '<', '&gt;': '>',
            '&quot;': '"', '&#39;': "'", '&ldquo;': '"', '&rdquo;': '"',
            '&lsquo;': "'", '&rsquo;': "'", '&middot;': 'Â·'
        }
        
        for entity, replacement in html_entities.items():
            text = text.replace(entity, replacement)
        
        # ç§»é™¤å¤šä½™ç©ºç™½
        text = re.sub(r'\s+', ' ', text)
        
        # æˆªæ–­åˆ°åˆé€‚é•¿åº¦
        return text.strip()[:500]
    
    def fetch_rss_news(self):
        """ä»RSSæºè·å–èµ„è®¯"""
        news_items = []
        
        for source in self.config['rss_sources']:
            try:
                print(f"æ­£åœ¨æŠ“å–: {source['name']}")
                feed = feedparser.parse(source['url'])
                
                for entry in feed.entries[:5]:  # æ¯ä¸ªæºå–5æ¡
                    if self._is_ai_related(entry.title):
                        # æ¸…ç†æ‘˜è¦å†…å®¹
                        raw_summary = entry.get('summary', entry.title)
                        cleaned_summary = self.clean_html_tags(raw_summary)
                        
                        news_item = {
                            'title': entry.title,
                            'summary': cleaned_summary,
                            'raw_summary': raw_summary,  # ä¿ç•™åŸå§‹ç”¨äºè°ƒè¯•
                            'link': entry.link,
                            'source': source['name'],
                            'published': entry.get('published', datetime.now().strftime('%Y-%m-%d %H:%M')),
                            'category': self._categorize_news(entry.title)
                        }
                        news_items.append(news_item)
                
                time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«
                
            except Exception as e:
                print(f"æŠ“å– {source['name']} å¤±è´¥: {e}")
                continue
        
        return news_items
    
    def _is_ai_related(self, title):
        """åˆ¤æ–­å†…å®¹æ˜¯å¦ä¸AI/æœºå™¨äººç›¸å…³"""
        if not title:
            return False
        
        text = title.lower()
        keywords = [
            'ai', 'äººå·¥æ™ºèƒ½', 'æœºå™¨å­¦ä¹ ', 'æ·±åº¦å­¦ä¹ ', 'ç¥ç»ç½‘ç»œ',
            'æœºå™¨äºº', 'robotics', 'robotic', 'å¤§æ¨¡å‹', 'gpt',
            'è‡ªåŠ¨é©¾é©¶', 'æ— äººé©¾é©¶', 'æ™ºèƒ½é©¾é©¶', 'llm',
            'è®¡ç®—æœºè§†è§‰', 'å›¾åƒè¯†åˆ«', 'è¯­éŸ³è¯†åˆ«', 'nlu',
            'æ™ºèƒ½å®¶å±…', 'ç‰©è”ç½‘', 'iot', 'æ™ºèƒ½ç¡¬ä»¶'
        ]
        return any(keyword in text for keyword in keywords)
    
    def _categorize_news(self, title):
        """æ ¹æ®æ ‡é¢˜åˆ†ç±»"""
        title_lower = title.lower()
        
        categories = {
            'åŒ»ç–—å¥åº·': ['åŒ»ç–—', 'å¥åº·', 'åŒ»ç”Ÿ', 'åŒ»é™¢', 'è¯Šæ–­', 'ç—…ç†'],
            'æœºå™¨äºº': ['æœºå™¨äºº', 'robotics', 'robotic', 'æœºæ¢°è‡‚', 'æ— äººæœº'],
            'è‡ªåŠ¨é©¾é©¶': ['é©¾é©¶', 'è‡ªåŠ¨', 'æ— äºº', 'æ±½è½¦', 'äº¤é€š'],
            'èŠ¯ç‰‡ç¡¬ä»¶': ['èŠ¯ç‰‡', 'gpu', 'tpu', 'ç¡¬ä»¶', 'åŠå¯¼ä½“'],
            'å¤§æ¨¡å‹': ['å¤§æ¨¡å‹', 'llm', 'gpt', 'æ–‡å¿ƒ', 'é€šä¹‰'],
            'æ•™è‚²': ['æ•™è‚²', 'å­¦ä¹ ', 'åŸ¹è®­', 'è¯¾ç¨‹'],
            'é‡‘è': ['é‡‘è', 'é“¶è¡Œ', 'æŠ•èµ„', 'è¯åˆ¸', 'ä¿é™©']
        }
        
        for category, keywords in categories.items():
            if any(keyword in title_lower for keyword in keywords):
                return category
        
        return 'AIé€šç”¨'
    
    def save_news(self, news_items):
        """ä¿å­˜èµ„è®¯åˆ°æ–‡ä»¶"""
        today = datetime.now().strftime('%Y-%m-%d')
        filename = f'output/daily/news_{today}.json'
        
        # æ·»åŠ è´¨é‡æ£€æŸ¥æ ‡è®°
        for item in news_items:
            item['quality_score'] = self._calculate_quality_score(item)
            item['collected_at'] = datetime.now().isoformat()
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(news_items, f, ensure_ascii=False, indent=2)
        
        print(f"å·²ä¿å­˜ {len(news_items)} æ¡èµ„è®¯åˆ° {filename}")
        return filename
    
    def _calculate_quality_score(self, item):
        """è®¡ç®—å†…å®¹è´¨é‡åˆ†æ•°"""
        score = 0
        
        # æ ‡é¢˜é•¿åº¦é€‚ä¸­åŠ åˆ†
        title_len = len(item.get('title', ''))
        if 20 <= title_len <= 50:
            score += 2
        elif 10 <= title_len < 20 or 50 < title_len <= 80:
            score += 1
        
        # æ‘˜è¦é•¿åº¦é€‚ä¸­åŠ åˆ†
        summary_len = len(item.get('summary', ''))
        if 100 <= summary_len <= 300:
            score += 2
        
        # åŒ…å«æ•°å­—åŠ åˆ†ï¼ˆé€šå¸¸æœ‰æ•°æ®æ”¯æ’‘ï¼‰
        if re.search(r'\d+', item.get('title', '') + item.get('summary', '')):
            score += 1
        
        # æ¥æºæƒå¨æ€§åŠ åˆ†
        authoritative_sources = ['æœºå™¨ä¹‹å¿ƒ', 'é‡å­ä½', 'MIT', 'IEEE']
        if item.get('source') in authoritative_sources:
            score += 2
        
        return min(score, 5)  # æœ€é«˜5åˆ†

def main():
    collector = NewsCollector()
    
    print("å¼€å§‹æ”¶é›†AIæœºå™¨äººèµ„è®¯...")
    print(f"èµ„è®¯æºæ•°é‡: {len(collector.config['rss_sources'])}")
    
    all_news = []
    all_news.extend(collector.fetch_rss_news())
    
    # å»é‡ï¼ˆåŸºäºæ ‡é¢˜ï¼‰
    unique_news = []
    seen_titles = set()
    
    for news in all_news:
        title = news['title']
        if title not in seen_titles:
            seen_titles.add(title)
            unique_news.append(news)
    
    # æŒ‰è´¨é‡åˆ†æ•°æ’åº
    unique_news.sort(key=lambda x: x.get('quality_score', 0), reverse=True)
    
    # ä¿å­˜
    collector.save_news(unique_news[:10])  # å–å‰10æ¡
    
    # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
    categories = {}
    for news in unique_news[:10]:
        cat = news.get('category', 'æœªçŸ¥')
        categories[cat] = categories.get(cat, 0) + 1
    
    print("\nğŸ“Š æ”¶é›†ç»Ÿè®¡:")
    print(f"æ€»æ”¶é›†æ•°: {len(unique_news)}")
    print(f"ç²¾é€‰æ•°: {min(10, len(unique_news))}")
    print("åˆ†ç±»åˆ†å¸ƒ:")
    for cat, count in categories.items():
        print(f"  {cat}: {count}æ¡")
    
    # ä¿å­˜åˆ°data.jsonä¾›å‘¨æŠ¥ä½¿ç”¨
    with open('output/data.json', 'w', encoding='utf-8') as f:
        json.dump({
            'last_updated': datetime.now().isoformat(),
            'news_count': len(unique_news),
            'news': unique_news[:10],
            'categories': categories
        }, f, ensure_ascii=False, indent=2)

if __name__ == '__main__':
    main()
