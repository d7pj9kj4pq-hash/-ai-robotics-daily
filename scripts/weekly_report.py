#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç”Ÿæˆå‘¨æŠ¥
"""

import json
import os
from datetime import datetime, timedelta

def generate_weekly_report():
    """ç”Ÿæˆå‘¨åº¦æŠ¥å‘Š"""
    print("å¼€å§‹ç”Ÿæˆå‘¨æŠ¥...")
    
    # è·å–è¿‡å»7å¤©çš„æ•°æ®
    weekly_news = []
    
    for i in range(7):
        date = (datetime.now() - timedelta(days=i)).strftime('%Y-%m-%d')
        file_path = f'output/daily/processed_{date}.json'
        
        if os.path.exists(file_path):
            with open(file_path, 'r', encoding='utf-8') as f:
                daily_news = json.load(f)
                weekly_news.extend(daily_news)
    
    if not weekly_news:
        print("æœ¬å‘¨æ²¡æœ‰æ•°æ®")
        return
    
    # è®¡ç®—ç»Ÿè®¡æ•°æ®
    source_counts = {}
    for item in weekly_news:
        source = item.get('source', 'æœªçŸ¥')
        source_counts[source] = source_counts.get(source, 0) + 1
    
    # æŒ‰æ ‡é¢˜å…³é”®è¯åˆ†ç±»
    categories = {
        'AIå¤§æ¨¡å‹': 0,
        'æœºå™¨äºº': 0,
        'è‡ªåŠ¨é©¾é©¶': 0,
        'èŠ¯ç‰‡ç¡¬ä»¶': 0,
        'å…¶ä»–': 0
    }
    
    for item in weekly_news:
        title = item.get('title', '').lower()
        if any(keyword in title for keyword in ['gpt', 'å¤§æ¨¡å‹', 'llm']):
            categories['AIå¤§æ¨¡å‹'] += 1
        elif 'æœºå™¨äºº' in title or 'robotics' in title:
            categories['æœºå™¨äºº'] += 1
        elif 'è‡ªåŠ¨é©¾é©¶' in title or 'æ— äººé©¾é©¶' in title:
            categories['è‡ªåŠ¨é©¾é©¶'] += 1
        elif any(keyword in title for keyword in ['èŠ¯ç‰‡', 'gpu', 'ç¡¬ä»¶']):
            categories['èŠ¯ç‰‡ç¡¬ä»¶'] += 1
        else:
            categories['å…¶ä»–'] += 1
    
    # ç”Ÿæˆå‘¨æŠ¥Markdown
    week_num = datetime.now().isocalendar()[1]
    start_date = (datetime.now() - timedelta(days=6)).strftime('%Y-%m-%d')
    end_date = datetime.now().strftime('%Y-%m-%d')
    
    markdown = f"""# ğŸ“Š AIä¸æœºå™¨äººå‘¨æŠ¥ ç¬¬{week_num}å‘¨

**ç»Ÿè®¡å‘¨æœŸ**: {start_date} è‡³ {end_date}
**èµ„è®¯æ€»æ•°**: {len(weekly_news)} æ¡

## ğŸ“ˆ æœ¬å‘¨æ•°æ®æ¦‚è§ˆ

### èµ„è®¯æ¥æºåˆ†å¸ƒ
"""
    
    for source, count in source_counts.items():
        markdown += f"- **{source}**: {count} æ¡\n"
    
    markdown += "\n### å†…å®¹åˆ†ç±»ç»Ÿè®¡\n"
    for category, count in categories.items():
        markdown += f"- **{category}**: {count} æ¡\n"
    
    markdown += f"""
### è¶‹åŠ¿åˆ†æ
1. æœ¬å‘¨æœ€æ´»è·ƒæ¥æº: {max(source_counts, key=source_counts.get)}
2. æœ€çƒ­é—¨é¢†åŸŸ: {max(categories, key=categories.get)}
3. å¹³å‡æ¯å¤©èµ„è®¯æ•°: {len(weekly_news)//7} æ¡

## ğŸ† æœ¬å‘¨çƒ­é—¨èµ„è®¯ï¼ˆå‰5ï¼‰

"""
    
    # ç®€å•æŒ‰æ ‡é¢˜é•¿åº¦å’Œå…³é”®è¯è¯„åˆ†ï¼ˆå®é™…ä¸­å¯ä»¥æ›´å¤æ‚ï¼‰
    def calculate_score(item):
        score = 0
        title = item.get('title', '')
        # å…³é”®è¯åŠ åˆ†
        keywords = ['çªç ´', 'é‡å¤§', 'é¦–æ¬¡', 'é©å‘½æ€§', 'é‡ç£…']
        for keyword in keywords:
            if keyword in title:
                score += 3
        # æ ‡é¢˜é•¿åº¦åŠ åˆ†ï¼ˆé•¿æ ‡é¢˜é€šå¸¸æ›´è¯¦ç»†ï¼‰
        score += min(len(title) / 10, 5)
        return score
    
    weekly_news.sort(key=calculate_score, reverse=True)
    
    for i, item in enumerate(weekly_news[:5], 1):
        markdown += f"""### {i}. {item['title']}

**æ¥æº**: {item.get('source', 'æœªçŸ¥')}
**å‘å¸ƒæ—¶é—´**: {item.get('published', 'æœªçŸ¥')}

**æ‘˜è¦**: {item.get('ai_summary', item.get('summary', 'æ— æ‘˜è¦'))[:150]}...

[æŸ¥çœ‹åŸæ–‡]({item.get('link', '#')})

---
"""
    
    # ä¿å­˜å‘¨æŠ¥
    os.makedirs('output/weekly', exist_ok=True)
    report_file = f'output/weekly/report_week{week_num}.md'
    
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(markdown)
    
    print(f"å‘¨æŠ¥å·²ç”Ÿæˆ: {report_file}")
    return report_file

if __name__ == '__main__':
    generate_weekly_report()
